{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec with SFCRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Willem Jan\n",
      "[nltk_data]     Willemse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Willem Jan\n",
      "[nltk_data]     Willemse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore', category = UserWarning, module = 'gensim')\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read SFCRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'EN'\n",
    "local_path = '../SFCR_data/'\n",
    "if not(os.path.isfile(local_path + 'SFCRs_' + language + '.dat')):\n",
    "    print(\"Files not found.\")\n",
    "else:\n",
    "    with open(local_path + 'SFCRs_' + language + '.dat', 'rb') as fp:\n",
    "        documents = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text8 / wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "gensim.downloader.BASE_DIR = '../../10_central_data/gensim-data'\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "#dataset = api.load(\"wiki-english-20171001\")\n",
    "\n",
    "pages_wiki = [d for d in dataset]\n",
    "\n",
    "pages_wiki = pages_wiki\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1701"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read legislation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "da_path = '../../../../../10_central_data/legislation/'\n",
    "DA = dict()\n",
    "art_dict= dict({'EN': ['Article',   'pre']})\n",
    "da_file = open(da_path + \"Delegated_Acts_\" + \"EN\" + \".txt\", \"rb\")\n",
    "DA[language] = da_file.read().decode('utf-8')\n",
    "da_file.close()\n",
    "def retrieve_article(language, article_num):\n",
    "    method = art_dict[language][1]\n",
    "    if method == 'pre':\n",
    "        string = art_dict[language][0] + ' ' + str(article_num) + ' (.*?)' + art_dict[language][0] + ' ' + str(article_num + 1)\n",
    "    elif method == 'post':\n",
    "        string = str(article_num) + ' ' + art_dict[language][0] + '(.*?)' + str(article_num + 1) + ' ' + art_dict[language][0]\n",
    "    elif method == 'postdot':\n",
    "        string = str(article_num) + '. ' + art_dict[language][0] + '(.*?)' + str(article_num + 1) + '. ' + art_dict[language][0]\n",
    "    r = re.compile(string, re.DOTALL)\n",
    "    result = ' '.join(r.search(DA[language])[1].split())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_text = []\n",
    "for article in range(1,381):\n",
    "    da_text.append(retrieve_article('EN', article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess with NLTK and Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model\n",
    "nlp = spacy.load('en', disable = ['parser', 'ner'])\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_wiki = []\n",
    "#for page in pages_wiki:\n",
    "#    for item in page['section_texts']:\n",
    "#        sent_list = nltk.tokenize.sent_tokenize(item)\n",
    "#        sentences_wiki.extend(sent_list)\n",
    "\n",
    "sentences_wiki = pages_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_sfcr = []\n",
    "for document in documents:\n",
    "    sent_list = nltk.tokenize.sent_tokenize(document)\n",
    "    sentences_sfcr.extend(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_da = []\n",
    "for article in da_text:\n",
    "    sent_list = nltk.tokenize.sent_tokenize(article)\n",
    "    sentences_da.extend(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lemmatized = lemmatization(sentences_wiki, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_sfcr = remove_stopwords(sentences_sfcr)\n",
    "sentences_da = remove_stopwords(sentences_da)\n",
    "sentences_wiki = remove_stopwords(sentences_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sfcr documents: 395\n",
      "Number of sfcr sentences: 287632\n",
      "Number of words: 5236154\n",
      "\n",
      "\n",
      "Number of da sentences: 27883\n",
      "Number of words: 668126\n",
      "\n",
      "\n",
      "Number of wiki sentences: 1701\n",
      "Number of words: 10753778\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sfcr documents: \" + str(len(documents)))\n",
    "print(\"Number of sfcr sentences: \" + str(len(sentences_sfcr)))\n",
    "print(\"Number of words: \" + str(sum([len(word) for word in sentences_sfcr])))\n",
    "print(\"\\n\")\n",
    "print(\"Number of da sentences: \" + str(len(sentences_da)))\n",
    "print(\"Number of words: \" + str(sum([len(word) for word in sentences_da])))\n",
    "print(\"\\n\")\n",
    "print(\"Number of wiki sentences: \" + str(len(sentences_wiki)))\n",
    "print(\"Number of words: \" + str(sum([len(word) for word in sentences_wiki])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_sfcr[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_total = sentences_sfcr + sentences_da + sentences_wiki\n",
    "\n",
    "bigram = gensim.models.Phrases(sentences_total, \n",
    "                               min_count = 10, \n",
    "                               threshold = 5) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[sentences_total], \n",
    "                                min_count = 10, \n",
    "                                threshold = 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(trigram[['technical', 'provisions'], ['risk', 'margins'], ['external', 'auditor']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(trigram[sentences_wiki], \n",
    "                 size = 300, \n",
    "                 window = 10, \n",
    "                 min_count = 10, \n",
    "                 workers = cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('provision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('compliance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('breach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '../nlp_data/'\n",
    "\n",
    "model.save(local_path + 'text8model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SFCR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_sfcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('text8model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(trigram[sentences_sfcr], update = True)\n",
    "\n",
    "model.train(trigram[sentences_sfcr],\n",
    "            total_examples = model.corpus_count,\n",
    "            epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('deferred_taxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('compliance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('technical_provisions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('breach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.words_closer_than('technical_provisions', 'risk_margin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with legislation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(sentences_da, update = True)\n",
    "model.train(sentences_da,\n",
    "            total_examples = model.corpus_count,\n",
    "            epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.words_closer_than('remuneration', 'bonus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top k words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get a list of all words\n",
    "all_words_sfcr = [word for item in sentences_sfcr for word in item]\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist_sfcr = nltk.FreqDist(all_words_sfcr)\n",
    "print(\"Number of unique words: \" +str(len(fdist_sfcr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose k and visually inspect the bottom 10 words of the top k\n",
    "k = 10000\n",
    "top_k_words_sfcr = fdist_sfcr.most_common(k)\n",
    "\n",
    "# define a function only to keep words in the top k words\n",
    "top_k_words_sfcr,_ = zip(*fdist_sfcr.most_common(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for word in top_k_words_da:\n",
    "    if word in model.wv.vocab:\n",
    "        embeddings.append(model.wv[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_2d = TSNE(perplexity = 40, \n",
    "               n_components = 2, \n",
    "               init = 'pca', \n",
    "               n_iter = 5000, \n",
    "               learning_rate = 20,\n",
    "               random_state = 0)\n",
    "\n",
    "embeddings_2d = tsne_2d.fit_transform(embeddings)\n",
    "\n",
    "def tsne_plot_2d(label, embeddings, words = [], a = 1):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, 1))\n",
    "    x = embeddings[:, 0]\n",
    "    y = embeddings[:, 1]\n",
    "\n",
    "    plt.scatter(x, y, c = colors, alpha = a, label = label)\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy = (x[i], y[i]), xytext = (5, 2), \n",
    "                     textcoords = 'offset points', ha = 'right', va = 'bottom', size = 10)\n",
    "    plt.legend(loc = 4)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot_2d('SFCR', embeddings_2d, words = top_k_words_da, a = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_da = [word for item in trigram[sentences_da] for word in item if word in model.wv.vocab]\n",
    "fdist_da = nltk.FreqDist(all_words_da)\n",
    "print(\"Number of unique words: \" +str(len(fdist_da)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4543\n",
    "top_k_words_da = fdist_da.most_common(k)\n",
    "\n",
    "# define a function only to keep words in the top k words\n",
    "top_k_words_da,_ = zip(*fdist_da.most_common(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_to_given([\"bdo\"], list(top_k_words_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_to_given([\"actuary\"], list(top_k_words_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_to_given([\"climate\"], list(top_k_words_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_to_given([\"tps\"], list(top_k_words_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_to_given([\"good\"], list(top_k_words_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_to_given()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in list(top_k_words_sfcr)[0:1000]:\n",
    "    if word in list(model.wv.vocab):\n",
    "        if word not in list(top_k_words_da):\n",
    "            print(word + \" --> \" + model.wv.most_similar_to_given([word], list(top_k_words_da)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_sfcr[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = {}\n",
    "for art in range(292, 310):\n",
    "    article = retrieve_article(\"EN\", art)\n",
    "    article = nltk.tokenize.sent_tokenize(article)\n",
    "    article = remove_stopwords(article)\n",
    "    value = 0\n",
    "    for item in trigram[article]:\n",
    "        item = [word for word in item if word in model.wv.vocab]\n",
    "        if item != []:\n",
    "            r = model.wv.n_similarity(sentences_sfcr[800], item)\n",
    "            value = value + r\n",
    "    i[art] = value / len(trigram[article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_article(\"EN\", 298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar_to_given('coc', ['governance', 'risk_margin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Governance article 258-275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " gensim.matutils.kullback_leibler(vec1, vec2, num_features=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_sfcr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sent for sent in sentences_da + sentences_sfcr + sentences_wiki if (sent != \"\") and (len(sent) > 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(\" \".join(line)), [i]) for i, line in enumerate(sentences)]\n",
    "test_corpus = [gensim.utils.simple_preprocess(\" \".join(line)) for i, line in enumerate(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21729"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size = 100, \n",
    "                 epochs = 100,\n",
    "                 workers = cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 53s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus))[0:10]:\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn = len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2767: 1,\n",
       "         737: 1,\n",
       "         8170: 1,\n",
       "         205: 1,\n",
       "         1057: 1,\n",
       "         244: 1,\n",
       "         1898: 1,\n",
       "         1818: 1,\n",
       "         3165: 1,\n",
       "         2513: 1})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (6803): «system governance general information system governance fit proper requirements risk management system including risk solvency assessment orsa internal control system compliance function internal audit function actuarial function outsourcing information risk profile underwriting risk market risk credit risk liquidity risk operational risk material risks information valuation solvency purposes assets technical provisions liabilities alternative methods valuation information capital management funds solvency capital requirement minimum capital requirement non compliance minimum capital requirement non compliance solvency capital requirement information templates summary summary solvency financial condition report sfcr contains quantitative qualitative information relating compre group group covering business performance system governance risk profile solvency valuation capital management group»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d100,n5,w5,mc5,s0.001,t8):\n",
      "\n",
      "MOST (18166, 0.9075143933296204): «sociedad limitada undertakings scope group group inclusion scope solvency criteria influence group supervision calculation method used used propor establish tional method type category ment share used date treatment code type mutual consoli group decision coun identification code id non capital dated voting level solvency art»\n",
      "\n",
      "MEDIAN (10927, 0.42256200313568115): «business name address organisation hawthorn life designated activity company swords business campus swords co dublin ireland supervisory authority central bank ireland insurance supervision division spencer dock north wall quay dublin ireland external auditor deloitte chartered accountants statutory audit firm hardwicke house hatch street dublin ireland group immediate parent undertaking controlling party hawthorn life columbia insurance company incorporated united states america»\n",
      "\n",
      "LEAST (18996, 0.044748228043317795): «rsa received legal opinions verifying compliance instruments follows subordinated guaranteed perpetual bond debt instrument classified innovative tier solvency purposes satisfies requirements criteria fca pra general prudential sourcebook genpru classified tier restricted solvency ii preference shares classified upper tier solvency purposes satisfy requirements genpru annex provisions genpru classified tier restricted solvency ii subordinated guaranteed perpetual bonds debt instrument subordinate guaranteed dated notes debt instrument classified lower tier solvency purposes satisfy requirements rules genpru classified tier instruments solvency ii»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "#doc_id = 3\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for doc_id in range(len(test_corpus)):\n",
    "    embeddings.append(model.infer_vector(test_corpus[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_2d = TSNE(perplexity = 2, \n",
    "               n_components = 2, \n",
    "               init = 'pca', \n",
    "               n_iter = 1000, \n",
    "               learning_rate = 20,\n",
    "               random_state = 0)\n",
    "\n",
    "embeddings_2d = tsne_2d.fit_transform(embeddings)\n",
    "\n",
    "def tsne_plot_2d(label, embeddings, words = [], a = 1):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, 1))\n",
    "    x = embeddings[:, 0]\n",
    "    y = embeddings[:, 1]\n",
    "\n",
    "    plt.scatter(x, y, c = colors, alpha = a, label = label)\n",
    "    #for i, word in enumerate(words):\n",
    "#        plt.annotate(word, xy = (x[i], y[i]), xytext = (5, 2), \n",
    "#                     textcoords = 'offset points', ha = 'right', va = 'bottom', size = 10)\n",
    "    plt.legend(loc = 4)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot_2d('DA', embeddings_2d, words = range(len(corpus)), a = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
