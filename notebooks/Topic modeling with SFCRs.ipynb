{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with SFCRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of topic modeling with SFCRs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read SFCRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'EN'\n",
    "local_path = '../SFCR_data/'\n",
    "if not(os.path.isfile(local_path + 'SFCRs_' + language + '.dat')):\n",
    "    print(\"Files not found.\")\n",
    "else:\n",
    "    with open(local_path + 'SFCRs_' + language + '.dat', 'rb') as fp:\n",
    "        documents = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess with NLTK and Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore', category = UserWarning, module = 'gensim')\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for document in documents:\n",
    "    sent_list = nltk.tokenize.sent_tokenize(document)\n",
    "    sentences.extend(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "sentences = remove_stopwords(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence for sentence in sentences if len(sentence) > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of documents: \" + str(len(documents)))\n",
    "print(\"Number of sentences: \" + str(len(sentences)))\n",
    "print(\"Number of words: \" + str(sum([len(word) for word in sentences])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get a list of all words\n",
    "all_words = [word for item in sentences for word in item]\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = nltk.FreqDist(all_words)\n",
    "print(\"Number of unique words: \" +str(len(fdist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose k and visually inspect the bottom 10 words of the top k\n",
    "k = 10000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function only to keep words in the top k words\n",
    "top_k_words,_ = zip(*fdist.most_common(k))\n",
    "top_k_words = set(top_k_words)\n",
    "def keep_top_k_words(text):\n",
    "    return [word for word in text if word in top_k_words]\n",
    "\n",
    "for idx in range(len(sentences)):\n",
    "    sentences[idx] = keep_top_k_words(sentences[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document length\n",
    "doc_lengths = [len(sentence) for sentence in sentences]\n",
    "\n",
    "print(\"length of list:\",len(doc_lengths),\n",
    "      \"\\naverage length:\", np.average(doc_lengths),\n",
    "      \"\\nminimum length:\", min(doc_lengths),\n",
    "      \"\\nmaximum length:\", max(doc_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data_lemmatized\"\n",
    "\n",
    "# Initialize spacy 'en' model\n",
    "nlp = spacy.load('en', disable = ['parser', 'ner'])\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(sentences, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#if not os.path.isfile(fname):\n",
    "#    filehandler = open(fname, 'wb') \n",
    "#    pickle.dump(data_lemmatized, filehandler)\n",
    "#else:\n",
    "#    filehandler = open(fname, 'rb') \n",
    "#    data_lemmatized = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = gensim.corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mallet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MALLET_HOME'] = 'C:\\\\mallet\\\\'\n",
    "mallet_path = r'C:\\\\mallet\\\\bin\\\\mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_list(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(\".\", end=\"\")\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, \n",
    "                                                 corpus=corpus, \n",
    "                                                 num_topics=num_topics, \n",
    "                                                 id2word=id2word,\n",
    "                                                 topic_threshold=0.0)\n",
    "        model_list.append(model)\n",
    "    return model_list\n",
    "\n",
    "def compute_coherence(dictionary, corpus, model_list):\n",
    "    coherence_values = []\n",
    "    for model in model_list:\n",
    "        print(\".\", end=\"\")\n",
    "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list = compute_model_list(dictionary = id2word,\n",
    "                                corpus = corpus,\n",
    "                                texts = data_lemmatized, \n",
    "                                start = 2, \n",
    "                                limit = 15,\n",
    "                                step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filehandler = open(local_path + \"lda_mallet_models\", 'wb') \n",
    "#pickle.dump(model_list, filehandler)\n",
    "#filehandler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cohere score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = compute_coherence(dictionary = id2word,\n",
    "                                     corpus = corpus,\n",
    "                                     model_list = model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=15; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "#plt.legend((\"coherence_values\"), loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal model\n",
    "optimal_model = model_list[8]\n",
    "print(optimal_model.print_topics(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
